---
title: "Shao2019_analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Shao2019_analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 12,
  fig.height = 6
)
options(tibble.print_min = 6L, tibble.print.max = 6L, digits = 3)
```

# Introduction
In this vignette we examine and model the Shao2019 data in more detail.

```{r setup}
library(parafac4microbiome)
library(dplyr)
library(ggplot2)
library(ggpubr)
```

# Processing the data cube
The data cube in Shao2019$data contains unprocessed counts. The function processDataCube() performs the processing of these counts with the following steps:

*   It performs feature selection based on the sparsityThreshold setting. Sparsity is here defined as the fraction of samples where a microbial abundance (ASV/OTU or otherwise) is zero. For Shao2019 we can take the delivery mode groups into account for feature selection. We do this by calculating the sparsity for each feature in each subject group and compare those against the sparsity threshold that we set. If a feature passes the threshold in either group, it is selected.
*   It performs a centered log-ratio transformation of each sample using the compositions::clr() function with a pseudo-count of one (on all features, prior to selection based on sparsity).
*   It centers and scales the three-way array. This is a complex subject, for which we refer to a [paper by Rasmus Bro and Age Smilde](https://doi.org/10.1002/cem.773). By centering across the subject mode, we make the subjects comparable to each other within each time point. Scaling within the feature mode avoids the PARAFAC model focussing on features with abnormally high variation.

The outcome of processing is a new version of the dataset. Please refer to the documentation of processDataCube() for more information.

```{r data processing}
processedShao = processDataCube(Shao2019, sparsityThreshold=0.9, considerGroups=TRUE, groupVariable="Delivery_mode", centerMode=1, scaleMode=2)
```

# Determining the correct number of components
A critical aspect of PARAFAC modelling is to determine the correct number of components. We have developed the function assessNumComponents() for this purpose. We give the function our new dataset and specify the minimum and maximum number of components to investigate and the number of randomly initialized models to try for each number of components.

Note: this vignette reflects a minimum working example for analysing this dataset due to computational limitations. Hence, we only look at 1-3 components with 5 random initializations each. These settings are not ideal for real datasets. Please refer to the documentation of assessNumComponents() for more information.

```{r Shao2019 num comp selection}
# Setup
# For computational purposes we deviate from the default settings
minNumComponents = 1
maxNumComponents = 3
numRepetitions = 5 # number of randomly initialized models
numFolds = 5 # number of jack-knifed models
maxit = 200
ctol= 1e-4 # this is a really bad setting but is needed to save computational time
numCores = 1

colourCols = c("Delivery_mode", "phylum", "")
legendTitles = c("Delivery mode", "Phylum", "")
xLabels = c("Subject index", "Feature index", "Time index")
legendColNums = c(3,5,0)
arrangeModes = c(TRUE, TRUE, FALSE)
continuousModes = c(FALSE,FALSE,TRUE)

# Assess the metrics to determine the correct number of components
assessment = assessNumComponents(processedShao$data, minNumComponents, maxNumComponents, numRepetitions, ctol=ctol, maxit=maxit, numCores=numCores)
```

We will now inspect the output plots of interest for Shao2019.

```{r overview plot}
assessment$plots$overview
```
The overview plots shows that we can explain ~17% of the variation in a three-component model. That is quite low. The CORCONDIA for that number of components is ~99.8 or higher, which is well above the minimum requirement of 60. 

```{r TCC}
assessment$plots$TCC[[2]]
assessment$plots$TCC[[3]]
```

The Tucker Congruence Coefficients show us how similar the loading vectors in the same mode are to each other, between the various components. A value >=0.85 has been described as problematic in the literature. Neither in the two-component model, nor in the three-component model are the loadings very similar to each other.

# Jack-knifed models
Next, we investigate the stability of the models when jack-knifing out samples. This will give us more information to choose between 2 or 3 components.

```{r model stability}
modelStabilityCheck(processedShao, numComponents=2, numFolds=numFolds, considerGroups=TRUE,
                               groupVariable="Delivery_mode", colourCols, legendTitles, xLabels, legendColNums, arrangeModes,
                               continuousModes, ctol=ctol, maxit=maxit, numCores=numCores)$plot
modelStabilityCheck(processedShao, numComponents=3, numFolds=numFolds, considerGroups=TRUE,
                               groupVariable="Delivery_mode", colourCols, legendTitles, xLabels, legendColNums, arrangeModes,
                               continuousModes, ctol=ctol, maxit=maxit, numCores=numCores)$plot
```
Both models are quite unstable. The models likely need more iterations or a more strict tolerance to converge properly (see documentation of assessNumComponents and modelStabilityCheck for more info). For simplicity's sake we select two components here, but refer to the parafac4microbiome paper for outcome when a more rigorous check is performed.

# Model selection
We have decided that a two-component model is the most appropriate for the Shao2019 dataset. We can now select one of the random initializations from the assessNumComponents() output as our final model. We're going to select the random initialisation that corresponded the maximum amount of variation explained for three components.

```{r model selection}
numComponents = 2
modelChoice = which(assessment$metrics$varExp[,numComponents] == max(assessment$metrics$varExp[,numComponents]))
finalModel = assessment$models[[numComponents]][[modelChoice]]
```

Finally, we visualize the model using plotPARAFACmodel().

```{r model plot}
plotPARAFACmodel(finalModel, processedShao, colourCols, legendTitles, xLabels, legendColNums, arrangeModes,
  continuousModes = c(FALSE,FALSE,TRUE),
  overallTitle = "Shao PARAFAC model")
```
